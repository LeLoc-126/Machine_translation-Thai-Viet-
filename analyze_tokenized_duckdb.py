import duckdb
import numpy as np
import matplotlib.pyplot as plt
import json
from pathlib import Path
from collections import Counter
import pickle
import seaborn as sns
from transformers import AutoTokenizer  # Náº¿u muá»‘n giáº£i mÃ£ token (náº¿u cáº§n)

# ğŸ“‚ Káº¿t ná»‘i Ä‘áº¿n DuckDB
db_path = "/home/leloc/Document/USTH/Thesis/Data/translation.db"  # ÄÆ°á»ng dáº«n DB
table_name = "translations"  # TÃªn báº£ng trong DB
con = duckdb.connect(db_path)
df = con.execute(f"SELECT thai_input_ids, vi_input_ids FROM {table_name} LIMIT 1000").fetchdf()

# ğŸ§¹ Unpickle dá»¯ liá»‡u
def unpickle(col):
    return [pickle.loads(b) if isinstance(b, (bytes, bytearray)) else b for b in col]

thai_ids = unpickle(df["thai_input_ids"])
vi_ids = unpickle(df["vi_input_ids"])

# ğŸ‘‹ Äá»‹nh nghÄ©a ID cho padding token (thÆ°á»ng lÃ  1)
PAD_TOKEN_ID = 1

# ğŸ§¹ Loáº¡i bá» padding tokens trong cÃ¢u
def remove_padding(tokens):
    return [token for token in tokens if token != PAD_TOKEN_ID]

# ğŸ“ TÃ­nh Ä‘á»™ dÃ i cÃ¢u sau khi loáº¡i bá» padding
thai_lens = [len(remove_padding(x)) for x in thai_ids]
vi_lens = [len(remove_padding(x)) for x in vi_ids]
len_ratios = [t / v if v != 0 else 0 for t, v in zip(thai_lens, vi_lens)]

# ğŸ“Š Thá»‘ng kÃª Ä‘á»™ dÃ i cÃ¢u
stats = {
    "thai_input_ids": {
        "max": int(np.max(thai_lens)),
        "min": int(np.min(thai_lens)),
        "mean": float(np.mean(thai_lens)),
        "median": float(np.median(thai_lens)),
    },
    "vi_input_ids": {
        "max": int(np.max(vi_lens)),
        "min": int(np.min(vi_lens)),
        "mean": float(np.mean(vi_lens)),
        "median": float(np.median(vi_lens)),
    },
    "length_ratio (thai/vi)": {
        "max": float(np.max(len_ratios)),
        "min": float(np.min(len_ratios)),
        "mean": float(np.mean(len_ratios)),
        "median": float(np.median(len_ratios)),
    },
}

# ğŸ”¢ Thá»‘ng kÃª token
all_tokens = [token for row in thai_ids for token in remove_padding(row)]
token_counts = Counter(all_tokens)

# ğŸ“ ThÆ° má»¥c lÆ°u káº¿t quáº£
output_dir = Path("EDA_tokenize")
output_dir.mkdir(exist_ok=True)

# ğŸ“ˆ Váº½ biá»ƒu Ä‘á»“ Ä‘á»™ dÃ i cÃ¢u (Histogram)
plt.figure(figsize=(10, 6))
plt.hist(thai_lens, bins=50, alpha=0.6, color='skyblue', label="Thai", edgecolor='black')
plt.hist(vi_lens, bins=50, alpha=0.6, color='orange', label="Vietnamese", edgecolor='black')
plt.xlabel("Number of tokens (Sentence Length)")
plt.ylabel("Number of Sentences")
plt.title("Distribution of Sentence Lengths (Tokens)")
plt.legend()
plt.tight_layout()
plt.savefig(output_dir / "sentence_length_histogram.png")

# ğŸ“‰ Váº½ boxplot cho Ä‘á»™ dÃ i cÃ¢u
plt.figure(figsize=(10, 6))
plt.boxplot([thai_lens, vi_lens], labels=["Thai", "Vietnamese"], vert=False)
plt.xlabel("Number of tokens (Sentence Length)")
plt.title("Boxplot of Sentence Lengths (Tokens)")
plt.tight_layout()
plt.savefig(output_dir / "sentence_length_boxplot.png")

# ğŸ’¾ LÆ°u thá»‘ng kÃª vÃ o file JSON
stats_output_file = output_dir / "statistics.json"
with open(stats_output_file, "w", encoding="utf-8") as f:
    json.dump(stats, f, ensure_ascii=False, indent=2, default=lambda o: int(o) if isinstance(o, (np.integer,)) else float(o))

print("âœ… Process completed and results saved at:", output_dir)
print(f"Statistics have been saved to the file {stats_output_file}")
