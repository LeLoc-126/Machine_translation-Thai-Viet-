import duckdb
from transformers import AutoTokenizer
import torch
import pickle
from tqdm import tqdm

# Cáº¥u hÃ¬nh
db_path = "/sdd/lv01/leloc/translation_machine/translation.db"
tokenizer_path = "/sdd/lv01/leloc/translation_machine/nllb-1.3B-thai-extended-tokenizer"
batch_size = 10000
max_length = 128

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)

# Káº¿t ná»‘i DuckDB
con = duckdb.connect(db_path)

# Táº£i dá»¯ liá»‡u
df = con.execute("""
    SELECT rowid AS id, thai, viet
    FROM translations
""").fetchdf()

# LÃ m sáº¡ch dá»¯ liá»‡u
df["thai"] = df["thai"].fillna("").astype(str)
df["viet"] = df["viet"].fillna("").astype(str)

# Chuáº©n bá»‹ danh sÃ¡ch káº¿t quáº£ Ä‘á»ƒ cáº­p nháº­t
records = []

# ğŸ”  Tokenize Thai theo batch
print("ğŸ”  Tokenizing Thai text...")
for i in tqdm(range(0, len(df), batch_size), desc="ğŸ‡¹ğŸ‡­ Thai", unit="batch"):
    thai_batch = ["tha_Thai " + text for text in df["thai"].iloc[i:i+batch_size]]
    thai_tokens = tokenizer(
        thai_batch,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=max_length,
        return_attention_mask=True
    )
    thai_input_ids_blobs = [pickle.dumps(x.numpy()) for x in thai_tokens["input_ids"]]
    thai_attention_mask_blobs = [pickle.dumps(x.numpy()) for x in thai_tokens["attention_mask"]]

    # LÆ°u táº¡m
    for j, idx in enumerate(range(i, min(i + batch_size, len(df)))):
        records.append({
            "id": df.iloc[idx]["id"],
            "thai_input_ids": thai_input_ids_blobs[j],
            "thai_attention_mask": thai_attention_mask_blobs[j]
        })

# ğŸ”  Tokenize Viá»‡t theo batch vÃ  thÃªm vÃ o records
print("ğŸ”  Tokenizing Vietnamese text...")
for i in tqdm(range(0, len(df), batch_size), desc="ğŸ‡»ğŸ‡³ Viet", unit="batch"):
    viet_batch = df["viet"].iloc[i:i+batch_size].tolist()
    viet_tokens = tokenizer(
        viet_batch,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=max_length,
        return_attention_mask=True
    )
    vi_input_ids_blobs = [pickle.dumps(x.numpy()) for x in viet_tokens["input_ids"]]
    vi_attention_mask_blobs = [pickle.dumps(x.numpy()) for x in viet_tokens["attention_mask"]]

    # Cáº­p nháº­t thÃªm vÃ o records
    for j, idx in enumerate(range(i, min(i + batch_size, len(df)))):
        records[idx]["vi_input_ids"] = vi_input_ids_blobs[j]
        records[idx]["vi_attention_mask"] = vi_attention_mask_blobs[j]

# ğŸ”„ Cáº­p nháº­t DuckDB theo batch
print("ğŸ”„ Updating DuckDB...")
update_batches = [
    (
        r["thai_input_ids"],
        r["thai_attention_mask"],
        r["vi_input_ids"],
        r["vi_attention_mask"],
        r["id"]
    )
    for r in records
]

for i in tqdm(range(0, len(update_batches), batch_size), desc="ğŸ’¾ DB Update", unit="batch"):
    con.executemany("""
        UPDATE translations
        SET thai_input_ids = ?,
            thai_attention_mask = ?,
            vi_input_ids = ?,
            vi_attention_mask = ?
        WHERE rowid = ?
    """, update_batches[i:i+batch_size])

# ÄÃ³ng káº¿t ná»‘i
con.close()
print("âœ… Done.")
